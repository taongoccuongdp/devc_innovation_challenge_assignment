{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Week5_Reading_Basic NLP-Sentiment Analysis.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LyU_yrP9Owx7"},"source":["## Sentiment Analysis with Logistic Regression"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uAzG2re1PtV0"},"source":["This dataset is from a real contest of Text Processing.\n","\n","The task is to build a model that will determine the tone (positive, negative) of the text. To do this, you will need to train the model on the existing data (train.csv). The resulting model will have to determine the class (neutral, positive, negative) of new texts. The dataset contains the following fields:\n","\n","| Field name | Meaning |\n","|------------|-----------|\n","| ItemID  | id of twit|\n","| Sentiment | sentiment (1-positive, 0-negative)|\n","| SentimentText | text of the twit|\n","\n","Let's first of all have a look at the data"]},{"cell_type":"code","metadata":{"id":"AWYiYMhSU8m0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1596177929029,"user_tz":-420,"elapsed":1018,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"ac9b0a7c-485c-44b6-b3eb-e133e9a21246"},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","sns.set_style(\"whitegrid\")"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"SvME2ywsO0Nx","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1596177962429,"user_tz":-420,"elapsed":34414,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"6dce0f42-d06a-482e-8f98-37aef0e4c1cb"},"source":["sentiment = pd.read_csv('https://raw.githubusercontent.com/dhminh1024/practice_datasets/master/twitter_sentiment_analysis.csv', encoding='latin-1')\n","sentiment.sample(5)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ItemID</th>\n","      <th>Sentiment</th>\n","      <th>SentimentText</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>33561</th>\n","      <td>33573</td>\n","      <td>0</td>\n","      <td>@aion_ayase pre-order from game 3 weeks ago an...</td>\n","    </tr>\n","    <tr>\n","      <th>64984</th>\n","      <td>64996</td>\n","      <td>0</td>\n","      <td>@bitter_cherry  At least Telemundo could leave...</td>\n","    </tr>\n","    <tr>\n","      <th>84872</th>\n","      <td>84884</td>\n","      <td>1</td>\n","      <td>@cbdesigns awww you're welcome  have to suppor...</td>\n","    </tr>\n","    <tr>\n","      <th>48889</th>\n","      <td>48901</td>\n","      <td>0</td>\n","      <td>@amancay sorry.. Didnt see that  .&amp;lt;3.</td>\n","    </tr>\n","    <tr>\n","      <th>94022</th>\n","      <td>94034</td>\n","      <td>1</td>\n","      <td>@coreen10 yup, it's awesome.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       ItemID  Sentiment                                      SentimentText\n","33561   33573          0  @aion_ayase pre-order from game 3 weeks ago an...\n","64984   64996          0  @bitter_cherry  At least Telemundo could leave...\n","84872   84884          1  @cbdesigns awww you're welcome  have to suppor...\n","48889   48901          0           @amancay sorry.. Didnt see that  .&lt;3.\n","94022   94034          1                      @coreen10 yup, it's awesome. "]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UdJgPnccP13V","colab":{"base_uri":"https://localhost:8080/","height":187},"executionInfo":{"status":"ok","timestamp":1596177962430,"user_tz":-420,"elapsed":34412,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"10f1e83e-44db-4147-8a01-90fc20d1ae9d"},"source":["sentiment.info()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 99989 entries, 0 to 99988\n","Data columns (total 3 columns):\n"," #   Column         Non-Null Count  Dtype \n","---  ------         --------------  ----- \n"," 0   ItemID         99989 non-null  int64 \n"," 1   Sentiment      99989 non-null  int64 \n"," 2   SentimentText  99989 non-null  object\n","dtypes: int64(2), object(1)\n","memory usage: 2.3+ MB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0eyPS4qgQTUr"},"source":["As we can see, the structure of a twit varies a lot between twit and twit. They have different lengths, letters, numbers, strange characters, etc. \n","\n","It is also important to note that **a lot** of words are not correctly spelled, for example, the word _\"Juuuuuuuuuuuuuuuuussssst\"_ or the word _\"sooo\"_\n","\n","This makes it hard to measure how positive or negative the words are within the twits.\n","\n","So we need a way of scoring the words such that words that appear in positive twits have a greater score than those that appear in negative twits.\n","\n","But first, how do we represent the twits as vectors we can input to our algorithm?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SMz35YKYRd-R"},"source":["### Bag of words"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JDA6FfQORhF1"},"source":["One thing we could do to represent the twits as equal-sized vectors of numbers is the following:\n","\n","* Create a list (vocabulary) with all the unique words in the whole corpus of twits. \n","* We construct a feature vector from each twit that contains the counts of how often each word occurs in the particular twit\n","\n","_Note that since the unique words in each twit represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros_\n","\n","Let's construct the bag of words. We will work with a smaller example for illustrative purposes, and in the end, we will work with our real data."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"H7m-TF9FQKP3","colab":{},"executionInfo":{"status":"ok","timestamp":1596177962431,"user_tz":-420,"elapsed":34411,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}}},"source":["twits = [\n","    'This is amazing!',\n","    'ML is the best, yes it is',\n","    'I am not sure about how this is going to end...'\n","]"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cXxKR5dVRq_O"},"source":["Let's import [CountVectorizer.](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) It'll help us to convert a collection of text documents to a matrix of token counts."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_i8bn8EoRoeB","colab":{},"executionInfo":{"status":"ok","timestamp":1596177962431,"user_tz":-420,"elapsed":34409,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Define an object of CountVectorizer() fit and transfom your twits into a 'bag'\n","count = CountVectorizer()\n","bag = count.fit_transform(twits)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b7RRV-gYR1yv","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"ok","timestamp":1596177962432,"user_tz":-420,"elapsed":34407,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"7f33a416-4cb2-47c5-d03f-27689c00758d"},"source":["# Find in document of CountVectorizer a function that show us list of feature names\n","count.get_feature_names()"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['about',\n"," 'am',\n"," 'amazing',\n"," 'best',\n"," 'end',\n"," 'going',\n"," 'how',\n"," 'is',\n"," 'it',\n"," 'ml',\n"," 'not',\n"," 'sure',\n"," 'the',\n"," 'this',\n"," 'to',\n"," 'yes']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rAzAWM0gR5E6"},"source":["As we can see from executing the preceding command, the vocabulary is stored in a Python array that maps the unique words to integer indices. Next, let's print the feature vectors that we just created:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B2opUD9OR5lD","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596177962433,"user_tz":-420,"elapsed":34406,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"ab97d22f-a090-4de5-a660-24c96e3a92cf"},"source":["# Call toarray() on your 'bag' to see the feature vectors\n","bag.toarray()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n","       [0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1],\n","       [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VifgJzQuSAlO"},"source":["**Quiz**: What is the index of the word 'is' and how many times it occurs in all three twits?"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0fPGxGk9Se0G"},"source":["Each index position in the feature vectors corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the first feature at index position 0 resembles the count of the word 'about', which only occurs in the last document. These values in the feature vectors are also called the **raw term frequencies**: `tf(t,d )` —the number of times a term `t` occurs in a document `d`."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KXxo5MKETMpB"},"source":["### How relevant are words? Term frequency-inverse document frequency\n","\n","We could use these raw term frequencies to score the words in our algorithm. There is a problem though: If a word is widespread in _all_ documents, then it probably doesn't carry a lot of information. To tackle this problem, we can use **term frequency-inverse document frequency**, which will reduce the score, the more frequent the word is across all twits. It is calculated like this:\n","\n","\\begin{equation*}\n","tf-idf(t,d) = tf(t,d) ~ idf(t,d)\n","\\end{equation*}\n","\n","_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n","\n","\\begin{equation*}\n","\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n","\\end{equation*}\n","\n","Where `n` is the total number of documents, and _df(t,d)_ is the number of documents where the term `t` appears. \n","\n","The `1` addition in the denominator is just to avoid zero terms for terms that appear in all documents, will not be entirely ignored. And the `log` ensures that low-frequency term doesn't get too much weight.\n","\n","Fortunately for us, `scikit-learn` does all those calculations for us:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9Ubq0BVJSgb-","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1596177962433,"user_tz":-420,"elapsed":34404,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"74eab910-864f-4399-96f6-9c999fd8b3cb"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer()\n","# Feed the tf-idf Vectorizer with twits using fit_transform()\n","tfidf_vec = tfidf.fit_transform(twits)\n","\n","# Formatting the number to 2 digits after the decimal point by showing on this notebook\n","np.set_printoptions(precision=2)\n","# To print array in one line\n","np.set_printoptions(linewidth=np.inf)\n","print(tfidf.get_feature_names())\n","print(tfidf_vec.toarray())"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['about', 'am', 'amazing', 'best', 'end', 'going', 'how', 'is', 'it', 'ml', 'not', 'sure', 'the', 'this', 'to', 'yes']\n","[[0.   0.   0.72 0.   0.   0.   0.   0.43 0.   0.   0.   0.   0.   0.55 0.   0.  ]\n"," [0.   0.   0.   0.4  0.   0.   0.   0.47 0.4  0.4  0.   0.   0.4  0.   0.   0.4 ]\n"," [0.33 0.33 0.   0.   0.33 0.33 0.33 0.2  0.   0.   0.33 0.33 0.   0.25 0.33 0.  ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JCoApsmLUIYJ"},"source":["## Data cleaning and manipulation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GU--SSvPUOKG"},"source":["Let's work on our real vocabulary"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pi8mjlxaUbpY"},"source":["### Removing stopwords"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yc6q-9NbUuVO"},"source":["**Stop words** are the most common words are meaningless in terms of sentiment: I, to, the, and... they don't give any information on positiveness or negativeness. They're basically noise that can most probably be eliminated. Therefore, it is a common practice to remove them when doing text analysis."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y0SJA2wiTbGr","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1596177963566,"user_tz":-420,"elapsed":35535,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"1511133e-da2b-4cdc-c0e6-11d694dd2731"},"source":["from collections import Counter\n","\n","vocab = Counter()\n","for twit in sentiment.SentimentText:\n","    for word in twit.split(' '):\n","        vocab[word] += 1\n","\n","vocab.most_common(20)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('', 123916),\n"," ('I', 32879),\n"," ('to', 28810),\n"," ('the', 28087),\n"," ('a', 21321),\n"," ('you', 21180),\n"," ('i', 15995),\n"," ('and', 14565),\n"," ('it', 12818),\n"," ('my', 12385),\n"," ('for', 12149),\n"," ('in', 11199),\n"," ('is', 11185),\n"," ('of', 10326),\n"," ('that', 9181),\n"," ('on', 9020),\n"," ('have', 8991),\n"," ('me', 8255),\n"," ('so', 7612),\n"," ('but', 7220)]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CXn9PepUUtzB","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1596177964219,"user_tz":-420,"elapsed":36186,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"d7a4817f-e001-44ed-b38a-3526408cdc30"},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kri9m-4gVETb","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1596177965001,"user_tz":-420,"elapsed":36966,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"42bfe45f-5a84-4edf-f1b3-86876044905d"},"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","vocab_reduced = Counter()\n","# Go through all of the items of vocab using vocab.items() and pick only words that are not in 'stop_words' \n","# and save them in vocab_reduced\n","for w, c in vocab.items():\n","    if not w in stop_words:\n","        vocab_reduced[w]=c\n","\n","vocab_reduced.most_common(20)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('', 123916),\n"," ('I', 32879),\n"," (\"I'm\", 6416),\n"," ('like', 5086),\n"," ('-', 4922),\n"," ('get', 4864),\n"," ('u', 4194),\n"," ('good', 3953),\n"," ('love', 3494),\n"," ('know', 3472),\n"," ('go', 2990),\n"," ('see', 2868),\n"," ('one', 2787),\n"," ('got', 2774),\n"," ('think', 2613),\n"," ('&amp;', 2556),\n"," ('lol', 2419),\n"," ('going', 2396),\n"," ('really', 2287),\n"," ('im', 2200)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wZz4s9q-VWsb"},"source":["### Removing special characters"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-n8CS-9uVle2"},"source":["If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"slKWGWafVSQ1","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596177965001,"user_tz":-420,"elapsed":36964,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"fa3705a0-f504-4f9d-a81c-549b57ae5d9c"},"source":["import re \n","\n","def preprocessor(text):\n","    \"\"\" Return a cleaned version of text\n","    \"\"\"\n","    # Remove HTML markup\n","    text = re.sub('<[^>]*>', '', text)\n","    # Save emoticons for later appending\n","    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n","    # Remove any non-word character and append the emoticons,\n","    # removing the nose character for standarization. Convert to lower case\n","    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n","    \n","    return text\n","\n","# Create some random texts for testing the function preprocessor()\n","print(preprocessor('I like it :), |||<><>'))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["i like it  :)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"j2WTq9RgWEA7"},"source":["### Stemming\n","\n","We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like love, loving, etc. _Could_ express the same positivity. If that was the case, we would be having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **stemming**.\n","\n","We also need a _tokenizer_ to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does stemming:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-3B7XXLsVwtN","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1596177965002,"user_tz":-420,"elapsed":36963,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"06b273fd-0df9-4b05-d66d-62a9fb7dc5e3"},"source":["from nltk.stem import PorterStemmer\n","\n","porter = PorterStemmer()\n","\n","# Split a text into list of words\n","def tokenizer(text):\n","    return text.split()\n","\n","# Split a text into list of words and apply stemming technic\n","def tokenizer_porter(text):\n","    return [porter.stem(word) for word in text.split()]\n","\n","# Testing\n","print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n","print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n","['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vpx3O0lSWMRO"},"source":["### Train Logistic Regression"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pxiIwOffWOw-","colab":{},"executionInfo":{"status":"ok","timestamp":1596177965003,"user_tz":-420,"elapsed":36962,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","X = sentiment['SentimentText']\n","y = sentiment['Sentiment']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=102)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PIpejBPlWT7Q","colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"status":"ok","timestamp":1596177986203,"user_tz":-420,"elapsed":58160,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"00f0c702-91de-497f-9644-78a45aad7fc6"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf = TfidfVectorizer(stop_words=stop_words,\n","                        tokenizer=tokenizer_porter,\n","                        preprocessor=preprocessor)\n","\n","# A pipeline is what chains several steps together, once the initial exploration is done. \n","# For example, some codes are meant to transform features — normalise numericals, or turn text into vectors, \n","# or fill up missing data, they are transformers; other codes are meant to predict variables by fitting an algorithm,\n","# they are estimators. Pipeline chains all these together which can then be applied to training data\n","clf = Pipeline([('vect', tfidf),\n","                ('clf', LogisticRegression(random_state=0))])\n","clf.fit(X_train, y_train)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Pipeline(memory=None,\n","         steps=[('vect',\n","                 TfidfVectorizer(analyzer='word', binary=False,\n","                                 decode_error='strict',\n","                                 dtype=<class 'numpy.float64'>,\n","                                 encoding='utf-8', input='content',\n","                                 lowercase=True, max_df=1.0, max_features=None,\n","                                 min_df=1, ngram_range=(1, 1), norm='l2',\n","                                 preprocessor=<function preprocessor at 0x7f518166c488>,\n","                                 smooth_idf=True,\n","                                 stop_words=['i', 'me', 'my', 'myself', '...\n","                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n","                                 tokenizer=<function tokenizer_porter at 0x7f518166cbf8>,\n","                                 use_idf=True, vocabulary=None)),\n","                ('clf',\n","                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n","                                    fit_intercept=True, intercept_scaling=1,\n","                                    l1_ratio=None, max_iter=100,\n","                                    multi_class='auto', n_jobs=None,\n","                                    penalty='l2', random_state=0,\n","                                    solver='lbfgs', tol=0.0001, verbose=0,\n","                                    warm_start=False))],\n","         verbose=False)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kKz3ngEmWaq_","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1596177990775,"user_tz":-420,"elapsed":62730,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"0e2582e8-97bb-43fd-cde8-ae4feddcfe0b"},"source":["from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n","\n","# Now apply those above metrics to evaluate your model\n","# Your code here\n","predictions = clf.predict(X_test)\n","print('accuracy:',accuracy_score(y_test,predictions))\n","print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n","print('classification report:\\n',classification_report(y_test,predictions))"],"execution_count":16,"outputs":[{"output_type":"stream","text":["accuracy: 0.7525752575257526\n","confusion matrix:\n"," [[5828 2877]\n"," [2071 9222]]\n","classification report:\n","               precision    recall  f1-score   support\n","\n","           0       0.74      0.67      0.70      8705\n","           1       0.76      0.82      0.79     11293\n","\n","    accuracy                           0.75     19998\n","   macro avg       0.75      0.74      0.75     19998\n","weighted avg       0.75      0.75      0.75     19998\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"70o0DCTtWm-9"},"source":["Finally, let's run some tests"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vA7H4ZJrWdrY","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1596177990776,"user_tz":-420,"elapsed":62729,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"179ca055-faf2-4f7c-b955-4ae43af17742"},"source":["twits = [\n","    \"I do not feel not bad\", # Phuc +1\n","    'This model is \"so good\" :))', # Long -1\n","    'we are who we are', # Nghi 0\n","    'its good to be bad sometimes', # PA +1\n","    'what a wonderful failure! (sarcasm :)))', #Phuc +1\n","    'People do not like the bad things', # Chi 0\n","    'We finally have the test result. You are positive', # Long +1\n","]\n","\n","preds = clf.predict_proba(twits)\n","\n","for i in range(len(twits)):\n","    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["I do not feel not bad --> Negative, Positive = [0.99 0.01]\n","This model is \"so good\" :)) --> Negative, Positive = [0.16 0.84]\n","we are who we are --> Negative, Positive = [0.39 0.61]\n","its good to be bad sometimes --> Negative, Positive = [0.55 0.45]\n","what a wonderful failure! (sarcasm :))) --> Negative, Positive = [0.31 0.69]\n","People do not like the bad things --> Negative, Positive = [0.79 0.21]\n","We finally have the test result. You are positive --> Negative, Positive = [0.2 0.8]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Cm38nJbbWxfn"},"source":["If we would like to use the classifier in another place, or just not train it again and again everytime, we can save the model in a pickle file:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cKtYI_QaWrOj","colab":{},"executionInfo":{"status":"ok","timestamp":1596177990776,"user_tz":-420,"elapsed":62727,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}}},"source":["import pickle\n","import os\n","\n","pickle.dump(clf, open('logisticRegression.pkl', 'wb'))"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F6dhu9N7Y3Jp"},"source":["And reload it in your app:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k1ePXZnQW2it","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1596177991292,"user_tz":-420,"elapsed":63242,"user":{"displayName":"Minh Do","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhnQ6aJ2YnaevyJzhii-qGws6Y17w-cwWLqF5iP=s64","userId":"12822549848477954436"}},"outputId":"9b97d6a0-6978-4da6-971f-cdab297441d5"},"source":["with open('logisticRegression.pkl', 'rb') as model:\n","    reload_model = pickle.load(model)\n","preds = reload_model.predict_proba(twits)\n","\n","for i in range(len(twits)):\n","    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["I do not feel not bad --> Negative, Positive = [0.99 0.01]\n","This model is \"so good\" :)) --> Negative, Positive = [0.16 0.84]\n","we are who we are --> Negative, Positive = [0.39 0.61]\n","its good to be bad sometimes --> Negative, Positive = [0.55 0.45]\n","what a wonderful failure! (sarcasm :))) --> Negative, Positive = [0.31 0.69]\n","People do not like the bad things --> Negative, Positive = [0.79 0.21]\n","We finally have the test result. You are positive --> Negative, Positive = [0.2 0.8]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UIj9GKy8Y9Pp"},"source":["**Well done!**"]}]}